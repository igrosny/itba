{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del método de Random Projections para el corpus de Twenty News Groups\n",
    "\n",
    "A continuación se implementará el método propuesto en este [paper](./random-indexing-dr-explained.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es generar la matriz $M'_{p*m}$ a partir de la matriz $M_{p*n}$ y la matriz de proyecciones $R_{n*m}$.\n",
    "Donde:\n",
    "\n",
    "* p = cantidad de documentos\n",
    "* n = cantidad de palabras en el vocabulario\n",
    "* m = cantidad de tópicos\n",
    "\n",
    "La matriz $M$ contiene una fila por documento y una columna por cáda término dentro del vocabulario.\n",
    "El elemento $M_{i,j}$ contendrá una medida de la cantidad de veces que aparece el término $j$ en el documento $i$. Para nuestro caso, esa medida será el tfidf. La matriz $M$ es sparsa ya que para un documento en particular, la gran mayoría de las palabras no aparecerá.\n",
    "\n",
    "La matriz $M'$ contiene una fila por documento y una columna por cada tópico. El elemento $M'_{i,j}$ contiene una medida de cuánto aporta el tópico $j$ a la construcción del documento $i$.\n",
    "\n",
    "El método propuesto por el paper consta en:\n",
    "\n",
    "1) Construcción de la matriz $M_{p*n}$ a partir del corpus generado por los artículos de Twenty News Groups. Esto se hará en un script de Python aparte para no ensuciar con código secundario esta notebook.\n",
    "\n",
    "2) Generación de la matriz de proyección $R$. Mas adelante se dará un detalle de la generación de esta matriz.\n",
    "\n",
    "3) Cálculo de la matriz $M'$ como la proyección definida por $R$ de la matriz $M$ en el espacio de menor dimensión.\n",
    "\n",
    "Una vez calculada esta transformación, podremos hacer el siguiente análisis:\n",
    "\n",
    "1) Tomar un palabra en particular del vocabulario (como un documento en el espacio de dimensión n cuyas componentes son todas cero salvo la componente que representa la palabra elegida que vale uno), transformarla con R y recuperar los k documentos vecinos.\n",
    "\n",
    "2) Tomar varias palabras del vocabulario (mismo procedimiento que el anterior, pero con más componentes en uno), transformarlas con R y recuperar los k documentos vecinos a este vector en el espacio de dimensión reducida.\n",
    "\n",
    "3) Tomar documentos de referencia de $M'$ y buscar sus k documentos mas cercanos en el espacio de dimensión reducida.\n",
    "\n",
    "Para cada punto definido anteriormente, se puede hacer una evaluación cualitativa de los resultados. ** Discutir un mecanismo de evaluación cuantitativo.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de la matriz $M$\n",
    "\n",
    "Para construir la matriz $M$ se utilizarán los artículos extraídos en esta [notebook](./Corpus que se utilizará para la comparación de métodos.ipynb).\n",
    "\n",
    "Primero se cargarán los artículos y luego se construirá la matriz $M$ utilizando el paquete tfidf de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cselmo/anaconda3/envs/OpLaDyn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import json\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('art_filt.txt', 'rb') as fp:\n",
    "    articulos = pickle.load(fp)\n",
    "with open ('art_filt_labels', 'rb') as fp:\n",
    "    labels = pickle.load(fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.219s.\n",
      "Se transformaron 11314 artículos, la cantidad de palabras en el vocabulario es: 28966\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,norm=\"l2\")\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(articulos)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "p=tfidf.shape[0]\n",
    "n=tfidf.shape[1]\n",
    "print(\"Se transformaron {} artículos, la cantidad de palabras en el vocabulario es: {}\".format(p,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de la matriz de proyección R\n",
    "\n",
    "Según lo visto en el paper de referencia [1], los elementos $r_{i,j}$ de la matriz R se calculan como:\n",
    "\n",
    "$$ r_{i,j} = \\sqrt{s}\\left\\{ \\begin{array}{rcl}\n",
    "1 & \\text{con probabilidad} & \\frac{1}{2s} \\\\ \n",
    "0 & \\text{con probabilidad} & 1-\\frac{1}{s} \\\\\n",
    "-1 & \\text{con probabilidad} & \\frac{1}{2s}\n",
    "\\end{array}\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de m mínimo para un error de 0.1 es:934\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "# Cálculo de m a partir del error\n",
    "error=0.1\n",
    "m=int(np.log(p)/(error**2))+1\n",
    "print(\"El valor de m mínimo para un error de {} es:{}\".format(error,m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.226s.\n"
     ]
    }
   ],
   "source": [
    "#Defino la densidad de la matriz según la definición del paper\n",
    "density=1/np.sqrt(n)\n",
    "#density=1/3\n",
    "\n",
    "t0 = time()\n",
    "rp=SparseRandomProjection(n_components=m, density=density)\n",
    "rp.fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x28966 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1067657 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(934, 28966)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtengo los artículos en su representación reducida\n",
    "rp.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_prime=np.dot(rp.components_,tfidf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 934)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_prime=M_prime.T\n",
    "M_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0496854095778152"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chequeo algún vector al azar, para comprobar que tenga norma uno\n",
    "np.linalg.norm(M_prime[3].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proceso los labels para que queden en formato one hot\n",
    "labels_oh = np.zeros((M_prime.shape[0], 20))\n",
    "labels_oh[np.arange(M_prime.shape[0]), labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_prime=M_prime.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.129s.\n",
      "Se transformaron 7532 artículos, la cantidad de palabras en el vocabulario es: 28966\n"
     ]
    }
   ],
   "source": [
    "# Genero el test-set\n",
    "\n",
    "with open ('art_filt_test.txt', 'rb') as fp:\n",
    "    articulos_test = pickle.load(fp)\n",
    "with open ('art_filt_test_labels.txt', 'rb') as fp:\n",
    "    labels_test = pickle.load(fp) \n",
    "tfidf_test = tfidf_vectorizer.transform(articulos_test)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "p=tfidf_test.shape[0]\n",
    "n=tfidf_test.shape[1]\n",
    "print(\"Se transformaron {} artículos, la cantidad de palabras en el vocabulario es: {}\".format(p,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0094327733981099\n"
     ]
    }
   ],
   "source": [
    "# Trasnformo el test set a R prima\n",
    "\n",
    "M_prime_test=np.dot(rp.components_,tfidf_test.T)\n",
    "M_prime_test=M_prime_test.T.toarray()\n",
    "#Chequeo algún vector al azar, para comprobar que tenga norma uno\n",
    "print(np.linalg.norm(M_prime_test[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proceso los labels para que queden en formato one hot\n",
    "labels_test_oh = np.zeros((M_prime_test.shape[0], 20))\n",
    "labels_test_oh[np.arange(M_prime_test.shape[0]), labels_test] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "tfidf=tfidf.toarray()\n",
    "tfidf_test=tfidf_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train loss=3.780539534 crossval loss=2.858926296\n",
      "Epoch: 0002 train loss=2.387215094 crossval loss=2.384835005\n",
      "Epoch: 0003 train loss=1.925596245 crossval loss=2.066501617\n",
      "Epoch: 0004 train loss=1.544671796 crossval loss=1.842457891\n",
      "Epoch: 0005 train loss=1.310173788 crossval loss=1.685601115\n",
      "Epoch: 0006 train loss=1.115884893 crossval loss=1.571999073\n",
      "Epoch: 0007 train loss=0.950864618 crossval loss=1.494475126\n",
      "Epoch: 0008 train loss=0.859440439 crossval loss=1.443039179\n",
      "Epoch: 0009 train loss=0.770634378 crossval loss=1.400556564\n",
      "Epoch: 0010 train loss=0.689865410 crossval loss=1.379191160\n",
      "Epoch: 0011 train loss=0.651552646 crossval loss=1.362631679\n",
      "Epoch: 0012 train loss=0.587656390 crossval loss=1.334321737\n",
      "Optimization Finished!\n",
      "Accuracy: 0.6188263\n",
      "done in 17.972s.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "from tensorboard import summary as summary_lib\n",
    "logs_path=\"log_dir\"\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 12\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "hidden_units=30\n",
    "\n",
    "# Network Parameters\n",
    "n_input =  M_prime.shape[1] # Vocab size \n",
    "n_classes = 20 # Twenty news groups # classes\n",
    "\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, n_input],name=\"X\")\n",
    "with tf.name_scope(\"labels\"):\n",
    "    Y = tf.placeholder(\"float\", [None, n_classes],name=\"Y\")\n",
    "\n",
    "# Construct model\n",
    "with tf.name_scope('Capa1'):\n",
    "    # Model\n",
    "    weights1= tf.Variable(tf.random_normal([n_input, hidden_units]),name=\"weights1\")\n",
    "    bias1= tf.Variable(tf.random_normal([hidden_units]),name=\"bias1\")\n",
    "    act1= tf.nn.sigmoid(tf.matmul(X,weights1)+bias1, name=\"activacion_1\")\n",
    "\n",
    "with tf.name_scope('Capa2'):\n",
    "    # Model\n",
    "    weights2= tf.Variable(tf.random_normal([hidden_units, n_classes]),name=\"weights2\")\n",
    "    bias2= tf.Variable(tf.random_normal([n_classes]),name=\"bias2\")\n",
    "    logits= tf.matmul(act1,weights2)+bias2\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "# Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y),name=\"costo\")\n",
    "with tf.name_scope('BGD'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name=\"optimizador\")\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    #pred = tf.nn.softmax(logits) # Softmax\n",
    "    acc_op = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(acc_op, tf.float32),name=\"acc_red_mean\")\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss_op)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc_op)\n",
    "# Merge all summaries into a single op\n",
    "tf.summary.histogram('histogram', weights1)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(M_prime.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next_batch(batch_size,M_prime,labels_oh)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c= sess.run([train_op, loss_op], feed_dict={Y: batch_y,\n",
    "                                                            X: batch_x})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            #batch_x, batch_y = next_batch(batch_size,M_prime,labels_oh)\n",
    "            #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            #run_metadata = tf.RunMetadata()\n",
    "            summary, test_cost,_ = sess.run([merged_summary_op,loss_op,acc_op],\n",
    "                                  feed_dict={X: M_prime_test, Y: labels_test_oh})#,\n",
    "                                  #options=run_options,\n",
    "                                  #run_metadata=run_metadata)\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train loss={:.9f} crossval loss={:.9f}\".format(avg_cost,test_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: M_prime_test, Y: labels_test_oh})) \n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train loss=3.491955540 crossval loss=3.109154224\n",
      "Epoch: 0002 train loss=2.822648222 crossval loss=2.738812208\n",
      "Epoch: 0003 train loss=2.421804791 crossval loss=2.453038931\n",
      "Epoch: 0004 train loss=2.058099600 crossval loss=2.194232941\n",
      "Epoch: 0005 train loss=1.727975141 crossval loss=1.967353582\n",
      "Epoch: 0006 train loss=1.437958284 crossval loss=1.777373075\n",
      "Epoch: 0007 train loss=1.207580090 crossval loss=1.624869823\n",
      "Epoch: 0008 train loss=1.008474644 crossval loss=1.508641243\n",
      "Epoch: 0009 train loss=0.855171972 crossval loss=1.413915753\n",
      "Epoch: 0010 train loss=0.732527908 crossval loss=1.342490554\n",
      "Epoch: 0011 train loss=0.632608278 crossval loss=1.286498785\n",
      "Epoch: 0012 train loss=0.542441987 crossval loss=1.243498325\n",
      "Optimization Finished\n",
      "Accuracy: 0.65361124\n",
      "done in 48.007s.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "from tensorboard import summary as summary_lib\n",
    "logs_path=\"log_dir_tfidf\"\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 12\n",
    "batch_size = 256\n",
    "display_step = 1\n",
    "hidden_units=5\n",
    "\n",
    "# Network Parameters\n",
    "n_input =  tfidf.shape[1] # Vocab size \n",
    "n_classes = 20 # Twenty news groups # classes\n",
    "\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, n_input],name=\"X\")\n",
    "with tf.name_scope(\"labels\"):\n",
    "    Y = tf.placeholder(\"float\", [None, n_classes],name=\"Y\")\n",
    "\n",
    "# Construct model\n",
    "with tf.name_scope('Capa1'):\n",
    "    # Model\n",
    "    weights1= tf.Variable(tf.random_normal([n_input, hidden_units]),name=\"weights1\")\n",
    "    bias1= tf.Variable(tf.random_normal([hidden_units]),name=\"bias1\")\n",
    "    act1= tf.nn.sigmoid(tf.matmul(X,weights1)+bias1, name=\"activacion_1\")\n",
    "\n",
    "with tf.name_scope('Capa2'):\n",
    "    # Model\n",
    "    weights2= tf.Variable(tf.random_normal([hidden_units, n_classes]),name=\"weights2\")\n",
    "    bias2= tf.Variable(tf.random_normal([n_classes]),name=\"bias2\")\n",
    "    logits= tf.matmul(act1,weights2)+bias2\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "# Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=logits, labels=Y),name=\"costo\")\n",
    "with tf.name_scope('BGD'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,name=\"optimizador\")\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "with tf.name_scope('Accuracy'):\n",
    "    # Accuracy\n",
    "    #pred = tf.nn.softmax(logits) # Softmax\n",
    "    acc_op = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    acc_op = tf.reduce_mean(tf.cast(acc_op, tf.float32),name=\"acc_red_mean\")\n",
    "    \n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", loss_op)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", acc_op)\n",
    "# Merge all summaries into a single op\n",
    "tf.summary.histogram('histogram', weights1)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "t0=time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # op to write logs to Tensorboard\n",
    "    summary_writer = tf.summary.FileWriter(logs_path, graph=sess.graph)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(M_prime.shape[0]/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next_batch(batch_size,tfidf,labels_oh)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c= sess.run([train_op, loss_op], feed_dict={Y: batch_y,\n",
    "                                                            X: batch_x})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            #batch_x, batch_y = next_batch(batch_size,M_prime,labels_oh)\n",
    "            #run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            #run_metadata = tf.RunMetadata()\n",
    "            summary, test_cost,_ = sess.run([merged_summary_op,loss_op,acc_op],\n",
    "                                  feed_dict={X: tfidf_test, Y: labels_test_oh})#,\n",
    "                                  #options=run_options,\n",
    "                                  #run_metadata=run_metadata)\n",
    "            summary_writer.add_summary(summary, epoch)\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"train loss={:.9f} crossval loss={:.9f}\".format(avg_cost,test_cost))\n",
    "    print(\"Optimization Finished\")\n",
    "\n",
    "    # Test model\n",
    "    pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(Y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: tfidf_test, Y: labels_test_oh})) \n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
